JavaScript Obfuscation Classification Pipeline

Project Overview

This project implements a machine learning pipeline to classify JavaScript scripts as obfuscated or non-obfuscated. The pipeline includes data loading, preprocessing, exploratory data analysis (EDA), feature extraction, model training, evaluation, and visualization. The script uses multiple machine learning models (Random Forest, Gradient Boosting, and SVM) to detect obfuscation patterns and evaluates their performance using metrics like accuracy, confusion matrices, and classification reports.


Dataset Source (https://www.kaggle.com/datasets/atifkhan89/java-script-obfuscated-samples)

Objectives





Load and preprocess JavaScript scripts from specified directories.



Perform Exploratory Data Analysis (EDA) to understand dataset characteristics.



Extract features using TF-IDF vectorization for machine learning.



Train and evaluate multiple classification models (RandomForest, GradientBoosting, SVM).



Visualize model performance and dataset characteristics (e.g., word counts, character frequencies, word clouds).



Save trained models and the TF-IDF vectorizer for future use.

Dataset

The dataset consists of JavaScript files stored in two directories:





Non-Obfuscated Scripts: Located at /kaggle/input/java-script-dataset/Final Project/JavascriptSamples.



Obfuscated Scripts: Located at /kaggle/input/java-script-dataset/Final Project/JavascriptSamplesObfuscated.

Each script is labeled as:





0 for non-obfuscated scripts.



1 for obfuscated scripts.

The processed dataset is saved as a CSV file (jav_script_datset.csv) in the /kaggle/working/ directory.

Prerequisites





Python: Version 3.8 or higher is recommended.



Python Libraries: Install the required libraries listed in requirements.txt using pip install -r requirements.txt.



Dataset: Ensure the JavaScript dataset is accessible in the specified directories or update the paths in the script.

Installation





Clone or download this repository.



Install the required Python libraries by running:

pip install -r requirements.txt



Ensure the dataset directories (JavascriptSamples and JavascriptSamplesObfuscated) are available in the specified paths or update the paths in the script.

File Structure





classification_pipeline.py: The main Python script containing the code for data loading, preprocessing, EDA, feature extraction, model training, evaluation, and visualization.



requirements.txt: Lists the required Python libraries.



README.md: This file, providing an overview and instructions for the project.



jav_script_datset.csv: The processed dataset (generated by the script).



Model files (e.g., RandomForest_model.pkl, vectorizer.pkl): Saved models and vectorizer (generated in /kaggle/working/).

Usage





Place the dataset directories in the correct location (e.g., /kaggle/input/java-script-dataset/Final Project/).



Update the file paths in the script if necessary:

data_path = "/path/to/save/jav_script_datset.csv"
non_obf_path = "/path/to/JavascriptSamples"
obf_path = "/path/to/JavascriptSamplesObfuscated"



Run the script in a Python environment (e.g., Jupyter Notebook, Kaggle, or local IDE):

python classification_pipeline.py



The script will:





Load and preprocess the JavaScript files.



Perform EDA (e.g., word count distributions, word clouds, character frequencies).



Train and evaluate machine learning models.



Save the processed dataset, trained models, and TF-IDF vectorizer.



Generate visualizations for dataset analysis and model performance.

Analysis Steps





Data Loading and Preprocessing:





Load JavaScript files from the specified directories and assign labels (0 for non-obfuscated, 1 for obfuscated).



Save the processed dataset as a CSV file with proper escaping for special characters.



Exploratory Data Analysis (EDA):





Visualize the distribution of obfuscated vs. non-obfuscated scripts using a count plot.



Analyze word count distributions with histograms.



Generate word clouds for non-obfuscated and obfuscated scripts to identify common words.



Plot the top 15 most frequent characters to detect patterns in script composition.



Feature Extraction:





Use TF-IDF Vectorization to convert scripts into numerical features (max 5000 features, English stopwords removed).



Model Training and Evaluation:





Split the dataset into 80% training and 20% testing sets.



Train three models: RandomForest, GradientBoosting, and SVM (linear kernel).



Evaluate models using accuracy, confusion matrices, and classification reports.



Save trained models and the TF-IDF vectorizer for future use.



Visualizations:





Plot a bar chart to compare model accuracies.



Generate heatmaps for confusion matrices to visualize classification performance.



Plot a line chart to compare model accuracies.

Visualizations





Count Plot: Distribution of non-obfuscated vs. obfuscated scripts.



Histogram: Word count distribution across scripts.



Word Clouds: Visual representation of frequent words in non-obfuscated and obfuscated scripts.



Bar Plot (Character Frequency): Top 15 most frequent characters in the dataset.



Bar Chart (Accuracy): Comparison of model accuracies.



Confusion Matrix Heatmaps: Classification performance for each model.

Requirements

See requirements.txt for the list of required Python libraries.

Notes





Ensure the dataset directories are correctly specified in the script.



The script handles file reading errors gracefully, ensuring robust data loading.



Obfuscated scripts may contain special characters or compressed code, so proper escaping is used when saving the CSV file.



The saved models and vectorizer can be used for inference on new JavaScript scripts.